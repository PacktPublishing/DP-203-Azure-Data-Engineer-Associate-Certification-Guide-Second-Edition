{"cells":[{"cell_type":"markdown","metadata":{},"source":["Use the following Azure Databricks storage setup block only if you are using Azure Databricks. You can refer to the instructions here to get started:\n","https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/azure-datalake-gen2-sp-access\n","\n","If you are using Synapse Spark and if your data is residing on the storage attached to the Synapse Spark workspace, you can skip the below storage setup section."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b78db953-876e-4b12-b0fa-edf527b269e9","showTitle":false,"title":""}},"outputs":[],"source":["%scala\n","val storageAccountName = \"<INSERT STORAGE ACCOUNT>\"\n","val fileSystemName = \"<INSERT CONTAINER NAME>\"\n","\n","val commonPath = \"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net\"\n","\n","# AAD Application Details\n","val appID = \"<INSERT APP ID>\"\n","val secret = \"<INSERT SECRET>\"\n","val tenantID = \"<INSERT TENANT ID>\"\n","\n","spark.conf.set(\"fs.azure.account.auth.type.\" + storageAccountName + \".dfs.core.windows.net\", \"OAuth\")\n","spark.conf.set(\"fs.azure.account.oauth.provider.type.\" + storageAccountName + \".dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.id.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + appID + \"\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.secret.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + secret + \"\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.\" + storageAccountName + \".dfs.core.windows.net\", \"https://login.microsoftonline.com/\" + tenantID + \"/oauth2/token\")\n","spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"true\")\n","dbutils.fs.ls(\"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net/\")\n","spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"fac7d340-ec5c-4fe4-a617-f617b272c923","showTitle":false,"title":""}},"outputs":[],"source":["%scala\n","\n","// Let us see how to write to a JSON file\n","import org.apache.spark.sql.{DataFrame, Row, SaveMode}\n","import org.apache.spark.sql.types.{StringType, IntegerType, StructField, StructType}\n","\n","val tripsCSVPath = commonPath + \"/batch/csv/trips\"\n","val faresParquetPath = commonPath + \"/batch/parquet/fares\"\n","\n","// Generate sample data\n","val tripSchema = new StructType()\n","      .add(\"tripId\",IntegerType)\n","      .add(\"driverId\",IntegerType)\n","      .add(\"customerId\",IntegerType)\n","      .add(\"cabId\",IntegerType)\n","      .add(\"tripDate\",StringType)\n","      .add(\"startLocation\",StringType)\n","      .add(\"endLocation\",StringType)\n","      \n","val tripData = Seq(\n","  Row(100, 200, 300, 400, \"20220101\", \"New York\", \"New Jersey\"),\n","  Row(101, 201, 301, 401, \"20220102\", \"Tempe\", \"Phoenix\"),\n","  Row(102, 202, 302, 402, \"20220103\", \"San Jose\", \"San Franciso\"),\n","  Row(103, 203, 303, 403, \"20220102\", \"New York\", \"Boston\"),\n","  Row(104, 204, 304, 404, \"20220103\", \"New York\", \"Washington\"),\n","  Row(105, 205, 305, 405, \"20220201\", \"Miami\", \"Fort Lauderdale\"),\n","  Row(106, 206, 306, 406, \"20220202\", \"Seattle\", \"Redmond\"),\n","  Row(107, 207, 307, 407, \"20220203\", \"Los Angeles\", \"San Diego\"),\n","  Row(108, 208, 308, 408, \"20220301\", \"Phoenix\", \"Las Vegas\"),\n","  Row(109, 209, 309, 409, \"20220302\", \"Washington\", \"Baltimore\"),\n","  Row(110, 210, 310, 410, \"20220303\", \"Dallas\", \"Austin\"),\n",")\n","\n","// Write Trips to CSV file\n","val tripDF = spark.createDataFrame(spark.sparkContext.parallelize(tripData),tripSchema)\n","tripDF.printSchema()\n","tripDF.show(false)\n","tripDF.write.mode(\"overwrite\").option(\"header\", \"true\").csv(tripsCSVPath)\n","\n","// Generate sample fares data\n","val fareSchema = new StructType()\n","      .add(\"tripId\",IntegerType)\n","      .add(\"fare\",IntegerType)\n","      .add(\"currency\",StringType)\n","\n","val fareData = Seq(\n","  Row(100, 100, \"USD\"),\n","  Row(101, 20, \"USD\"),\n","  Row(102, 25, \"USD\"),\n","  Row(103, 140, \"USD\"),\n","  Row(104, 340, \"USD\"),\n","  Row(105, 75, \"USD\"),\n","  Row(106, 50, \"USD\"),\n","  Row(107, 125, \"USD\"),\n","  Row(108, 40, \"USD\"),\n","  Row(109, 80, \"USD\"),\n","  Row(110, 160, \"USD\")\n",")\n","\n","// Write Trips to Parquet file\n","val faresDF = spark.createDataFrame(spark.sparkContext.parallelize(fareData),fareSchema)\n","faresDF.printSchema()\n","faresDF.show(false)\n","faresDF.write.mode(\"overwrite\").option(\"header\", \"true\").parquet(faresParquetPath)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"7004cec7-26c9-43d1-929f-1a2e5db59c37","showTitle":false,"title":""}},"outputs":[],"source":["%scala\n","//Sample Batch Transformation using ADB. Let us try this one in scala\n","import org.apache.spark.sql.{DataFrame, Row, SaveMode}\n","import org.apache.spark.sql.types.{StringType, IntegerType, StructField, StructType}\n","\n","val tripsCSVPath = commonPath + \"/batch/csv/trips/*\"\n","val faresParquetPath = commonPath + \"/batch/parquet/fares/*\"\n","val outputParquetPath = commonPath + \"/batch/parquet/output\"\n","\n","// Read  the Trip data (stored as CSV file) and the Fares data (stored as Parquet files)\n","val tripsSchema = new StructType()\n","      .add(\"tripId\",IntegerType)\n","      .add(\"driverId\",IntegerType)\n","      .add(\"customerId\",IntegerType)\n","      .add(\"cabId\",IntegerType)\n","      .add(\"tripDate\",IntegerType)\n","      .add(\"startLocation\",StringType)\n","      .add(\"endLocation\",StringType)\n","\n","val tripsCSV = spark.read.format(\"csv\")\n","      .option(\"header\", \"true\")\n","      .schema(tripsSchema)\n","      .load(tripsCSVPath)\n","tripsCSV.printSchema()\n","tripsCSV.show(false)\n","\n","val faresSchema = new StructType()\n","      .add(\"tripId\",IntegerType)\n","      .add(\"fare\",IntegerType)\n","      .add(\"currency\",StringType)\n","\n","val faresParquet = spark.read.format(\"parquet\")\n","            .schema(faresSchema)\n","            .load(faresParquetPath)\n","faresParquet.printSchema()\n","faresParquet.show(false)\n","\n","\n","\n","// Join them on the tripID and group by StartLocation.\n","val joinDF = tripsCSV.join(\n","faresParquet,tripsCSV(\"tripId\") === \n","      faresParquet(\"tripId\"),\"inner\")\n",".groupBy(\"startLocation\")\n",".sum(\"fare\");\n","\n","// Print the output table with columns: City and Fare\n","import org.apache.spark.sql.functions.col;\n","val outputDF = joinDF.select(col(\"startLocation\").alias(\"City\"),col(\"sum(fare)\").alias(\"Fare\"));\n","display(outputDF)\n","//\tFinally, write the output back to ADLS Gen2 under the transform/fares/out folder.\n","outputDF.write.mode(\"overwrite\").parquet(outputParquetPath)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f67d9a90-c40f-429f-ada0-8ad645ba453c","showTitle":false,"title":""}},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"SparkBatchJob-C9","notebookOrigID":188113580888598,"widgets":{}},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
