{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"c7ab3737-6159-48cc-a309-8a89455de4e8","showTitle":false,"title":""}},"source":["Use the following Azure Databricks storage setup block only if you are using Azure Databricks. You can refer to the instructions here to get started:\n","https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/azure-datalake-gen2-sp-access\n","\n","If you are using Synapse Spark and if your data is residing on the storage attached to the Synapse Spark workspace, you can skip the below storage setup section."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"16551fdb-a4b7-4f51-94f9-3e00e0bf3e69","showTitle":false,"title":""}},"outputs":[],"source":["%scala\n","val storageAccountName = \"<INSERT STORAGE ACCOUNT>\"\n","val fileSystemName = \"<INSERT CONTAINER NAME>\"\n","\n","val commonPath = \"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net\"\n","\n","# AAD Application Details\n","val appID = \"<INSERT APP ID>\"\n","val secret = \"<INSERT SECRET>\"\n","val tenantID = \"<INSERT TENANT ID>\"\n","\n","spark.conf.set(\"fs.azure.account.auth.type.\" + storageAccountName + \".dfs.core.windows.net\", \"OAuth\")\n","spark.conf.set(\"fs.azure.account.oauth.provider.type.\" + storageAccountName + \".dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.id.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + appID + \"\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.secret.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + secret + \"\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.\" + storageAccountName + \".dfs.core.windows.net\", \"https://login.microsoftonline.com/\" + tenantID + \"/oauth2/token\")\n","spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"true\")\n","dbutils.fs.ls(\"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net/\")\n","spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"3bb54392-dd29-4d01-9f66-adbda394fd03","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">Default Partitions: 8\n","Repartition Partitions: 3\n","Coalesce Partitions: 2\n","Range Partitions: 1\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">Default Partitions: 8\nRepartition Partitions: 3\nCoalesce Partitions: 2\nRange Partitions: 1\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["%python\n","import pyspark\n","\n","columnNames = [\"name\",\"license\",\"gender\",\"salary\"]\n","driverData = [\n","  ('Alice', 'A224455', 'Female', 3000),\n","  ('Bryan','B992244','Male',4000),\n","  ('Catherine','C887733','Female',2000),\n","  ('Daryl','D229988','Male',3000),\n","  ('Jenny','J663300','Female', 6000)\n","]\n","\n","# Create the Dataframe\n","df = spark.createDataFrame(data= driverData, schema = columnNames)\n","print(\"Default Partitions: \" + str(df.rdd.getNumPartitions()))\n","\n","# Using repartition\n","repartitionDF = df.repartition(3)\n","print(\"Repartition Partitions: \" + str(repartitionDF.rdd.getNumPartitions()))\n","\n","# Using coalesce\n","coalesceDF=df.coalesce(2)\n","print(\"Coalesce Partitions: \" + str(coalesceDF.rdd.getNumPartitions()))\n","\n","# Using reparitionByRange\n","repartitionRangeDF = df.repartitionByRange(1,'salary')\n","print(\"Range Partitions: \" + str(repartitionRangeDF.rdd.getNumPartitions()))\n","\n","# You can also use partitionBy and write to files\n","df.write.partitionBy(\"gender\",\"salary\").mode(\"overwrite\").parquet(commonPath + \"/parquet/driver/partition/\")\n"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"PartitioningUsingSpark-C5","notebookOrigID":188113580888533,"widgets":{}},"kernelspec":{"display_name":"python","name":"synapse_pyspark"},"language_info":{"name":"python"},"save_output":true,"synapse_widget":{"state":{},"version":"0.1"}},"nbformat":4,"nbformat_minor":0}
