{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use the following Azure Databricks storage setup block only if you are using Azure Databricks. You can refer to the instructions here to get started:\n",
        "https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/azure-datalake-gen2-sp-access\n",
        "\n",
        "If you are using Synapse Spark and if your data is residing on the storage attached to the Synapse Spark workspace, you can skip the below storage setup section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%scala\n",
        "val storageAccountName = \"<INSERT STORAGE ACCOUNT>\"\n",
        "val fileSystemName = \"<INSERT CONTAINER NAME>\"\n",
        "\n",
        "val commonPath = \"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net\"\n",
        "\n",
        "# AAD Application Details\n",
        "val appID = \"<INSERT APP ID>\"\n",
        "val secret = \"<INSERT SECRET>\"\n",
        "val tenantID = \"<INSERT TENANT ID>\"\n",
        "\n",
        "spark.conf.set(\"fs.azure.account.auth.type.\" + storageAccountName + \".dfs.core.windows.net\", \"OAuth\")\n",
        "spark.conf.set(\"fs.azure.account.oauth.provider.type.\" + storageAccountName + \".dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
        "spark.conf.set(\"fs.azure.account.oauth2.client.id.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + appID + \"\")\n",
        "spark.conf.set(\"fs.azure.account.oauth2.client.secret.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + secret + \"\")\n",
        "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.\" + storageAccountName + \".dfs.core.windows.net\", \"https://login.microsoftonline.com/\" + tenantID + \"/oauth2/token\")\n",
        "spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"true\")\n",
        "dbutils.fs.ls(\"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net/\")\n",
        "spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import org.apache.spark.sql.{DataFrame, Row, SaveMode}\n",
        "import org.apache.spark.sql.types.{StringType, IntegerType, StructField, StructType}\n",
        "\n",
        "val tripsParquetPath = commonPath + \"/hyperspace/trips/\"\n",
        "val driverParquetPath = commonPath + \"/hyperspace/driver/\"\n",
        "\n",
        "// Generate sample trips data\n",
        "val tripSchema = new StructType().add(\"tripId\", StringType).add(\"driverId\", StringType).add(\"customerId\",StringType).add(\"cabId\",StringType).add(\"tripDate\",StringType).add(\"startLocation\",StringType).add(\"endLocation\",StringType)\n",
        "\n",
        "val tripData = Seq(\n",
        "  Row(\"100\", \"200\", \"300\", \"400\", \"20240101\", \"New York\", \"New Jersey\"),\n",
        "  Row(\"101\", \"201\", \"301\", \"401\", \"20240102\", \"Tempe\", \"Phoenix\"),\n",
        "  Row(\"102\", \"202\", \"302\", \"402\", \"20240103\", \"San Jose\", \"San Franciso\"),\n",
        "  Row(\"103\", \"203\", \"303\", \"403\", \"20240102\", \"New York\", \"Boston\"),\n",
        "  Row(\"104\", \"204\", \"304\", \"404\", \"20240103\", \"New York\", \"Washington\"),\n",
        "  Row(\"105\", \"205\", \"305\", \"405\", \"20240201\", \"Miami\", \"Fort Lauderdale\"),\n",
        "  Row(\"106\", \"206\", \"306\", \"406\", \"20240202\", \"Seattle\", \"Redmond\"),\n",
        "  Row(\"107\", \"207\", \"307\", \"407\", \"20240203\", \"Los Angeles\", \"San Diego\"),\n",
        "  Row(\"108\", \"208\", \"308\", \"408\", \"20240301\", \"Phoenix\", \"Las Vegas\"),\n",
        "  Row(\"109\", \"209\", \"309\", \"409\", \"20240302\", \"Washington\", \"Baltimore\"),\n",
        "  Row(\"110\", \"210\", \"310\", \"410\", \"20240303\", \"Dallas\", \"Austin\"),\n",
        "  Row(\"111\", \"211\", \"311\", \"411\", \"20240303\", \"New York\", \"New Jersey\"),\n",
        "  Row(\"112\", \"212\", \"312\", \"412\", \"20240304\", \"New York\", \"Boston\"),\n",
        "  Row(\"113\", \"212\", \"312\", \"412\", \"20240401\", \"San Jose\", \"San Ramon\"),\n",
        "  Row(\"114\", \"212\", \"312\", \"412\", \"20240404\", \"San Jose\", \"Oakland\"),\n",
        "  Row(\"115\", \"212\", \"312\", \"412\", \"20240404\", \"Tempe\", \"Scottsdale\"),\n",
        "  Row(\"116\", \"212\", \"312\", \"412\", \"20240405\", \"Washington\", \"Atlanta\"),\n",
        "  Row(\"117\", \"212\", \"312\", \"412\", \"20240405\", \"Seattle\", \"Portland\"),\n",
        "  Row(\"118\", \"212\", \"312\", \"412\", \"20240405\", \"Miami\", \"Tampa\")\n",
        ")\n",
        "\n",
        "// Write Trips to Parquet\n",
        "val tripWriteDF = spark.createDataFrame(spark.sparkContext.parallelize(tripData),tripSchema)\n",
        "tripWriteDF.write.mode(\"overwrite\").parquet(tripsParquetPath)\n",
        "\n",
        "val driverSchema = new StructType().add(\"driverId\", StringType).add(\"name\", StringType).add(\"license\",StringType).add(\"gender\",StringType).add(\"salary\",IntegerType)\n",
        "\n",
        "val driverData = Seq(\n",
        "  Row(\"200\", \"Alice\", \"A224455\", \"Female\", 3000),\n",
        "  Row(\"202\", \"Bryan\",\"B992244\",\"Male\",4000),\n",
        "  Row(\"204\", \"Catherine\",\"C887733\",\"Female\",4000),\n",
        "  Row(\"208\", \"Daryl\",\"D229988\",\"Male\",3000),\n",
        "  Row(\"212\", \"Jenny\",\"J663300\",\"Female\", 5000)\n",
        ")\n",
        "// Write Driver to Parquet\n",
        "val driverWriteDF = spark.createDataFrame(spark.sparkContext.parallelize(driverData),driverSchema)\n",
        "driverWriteDF.write.mode(\"overwrite\").parquet(driverParquetPath)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "// Let us read back the files to check if the data is showing up correctly\n",
        "val tripsDF: DataFrame = spark.read.parquet(tripsParquetPath)\n",
        "val driverDF: DataFrame = spark.read.parquet(driverParquetPath)\n",
        "\n",
        "// Verify the data is available and correct\n",
        "tripsDF.show()\n",
        "driverDF.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "// Now let us try to join the tables and create a query, which we can later optimize using Hyperspace indexing\n",
        "val driverFilter: DataFrame = tripsDF.join(driverDF, tripsDF(\"driverId\") === driverDF(\"driverId\")).select(tripsDF(\"tripId\"), driverDF(\"name\"))\n",
        "driverFilter.show()\n",
        "\n",
        "driverFilter.explain(true)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "// Let us try the same query with Hypserspace enabled now\n",
        "\n",
        "// Create an instance of Hyperspace\n",
        "import com.microsoft.hyperspace._\n",
        "import com.microsoft.hyperspace.index._\n",
        "\n",
        "val hs: Hyperspace = Hyperspace()\n",
        "\n",
        "// Delete and vacuum the index if you are trying to rerun the query\n",
        "//hs.deleteIndex(\"TripIndex\")\n",
        "//hs.deleteIndex(\"DriverIndex\")\n",
        "//hs.vacuumIndex(\"TripIndex\")\n",
        "//hs.vacuumIndex(\"DriverIndex\")\n",
        "\n",
        "// Create the trips and driver indexes\n",
        "hs.createIndex(tripsDF, IndexConfig(\"TripIndex\", indexedColumns = Seq(\"driverId\"), includedColumns = Seq(\"tripId\")))\n",
        "hs.createIndex(driverDF, IndexConfig(\"DriverIndex\", indexedColumns = Seq(\"driverId\"), includedColumns = Seq(\"name\")))\n",
        "\n",
        "// List the indexes to check if the new indexes have been created\n",
        "hs.indexes.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "// Enable Hyperspace\n",
        "spark.enableHyperspace\n",
        "\n",
        "// Read back the same trip and driver parquet data into dataframes again\n",
        "val tripIndexDF: DataFrame = spark.read.parquet(tripsParquetPath)\n",
        "val driverIndexDF: DataFrame = spark.read.parquet(driverParquetPath)\n",
        "\n",
        "tripIndexDF.show(5)\n",
        "driverIndexDF.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "// Run a Join query again\n",
        "val filterJoin: DataFrame = tripIndexDF.join(driverIndexDF, tripIndexDF(\"driverId\") === driverIndexDF(\"driverId\")).select(tripIndexDF(\"tripId\"), driverIndexDF(\"name\"))\n",
        "filterJoin.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "// Check the comparision of the queryplan with and without Index\n",
        "\n",
        "spark.conf.set(\"spark.hyperspace.explain.displayMode\", \"html\")\n",
        "hs.explain(filterJoin)(displayHTML(_))"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "scala"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
