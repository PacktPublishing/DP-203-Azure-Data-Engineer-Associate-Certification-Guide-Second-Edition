{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Load the data from its source.\n",
        "df=spark.read.load(“/databricks/DP203/learning/books/book.delta\")\n",
        "# Write the data and save the table name as book.\n",
        "table_name = “book”\n",
        "df.write.saveAsTable(table_name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "sql"
        }
      },
      "outputs": [],
      "source": [
        "%sql\n",
        "\n",
        "-- Upserting to a table.\n",
        "\n",
        "CREATE OR REPLACE TEMP VIEW book_updates AS\n",
        "VALUES\n",
        "  (998, 'Into the World', 'John', 'Science Fiction', 9.99, 4.5),\n",
        "  (999, 'First Step', 'Adams', 'Romance', 7.99, 4.2),\n",
        "  (1000, 'Great War', 'Stephen', 'History', 19.99, 4.8),\n",
        "  (2001, 'Dream World', 'Saliken', 'Fantasy', 8.99, 4.4),\n",
        "  (2002, 'little boy in town', 'Suren', 'Science Fiction', 9.99, 4.9),\n",
        "  (2003, 'Hope you can', 'David', 'New-age', 14.99, 4.6)\n",
        "AS book_updates(id, title, author, genre, price, rating);\n",
        "\n",
        "MERGE INTO book\n",
        "USING book_updates\n",
        "ON book.id = book_updates.id\n",
        "WHEN MATCHED THEN \n",
        "  UPDATE SET \n",
        "    title = book_updates.title,\n",
        "    author = book_updates.author,\n",
        "    genre = book_updates.genre,\n",
        "    price = book_updates.price,\n",
        "    rating = book_updates.rating\n",
        "WHEN NOT MATCHED THEN \n",
        "  INSERT (id, title, author, genre, price, rating) \n",
        "  VALUES (book_updates.id, book_updates.title, book_updates.author, book_updates.genre, book_updates.price, book_updates.rating);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Reading a book Table\n",
        "book_df = spark.read.table(table_name)\n",
        "\n",
        "display(book_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Writing to a table To append\n",
        "book_df.write.mode(\"append\").saveAsTable(\"book\")\n",
        "\n",
        "# To overwrite\n",
        "book_df.write.mode(\"overwrite\").saveAsTable(\"book\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Using Optimze command to merge the small files\n",
        "\n",
        "from delta.tables import *\n",
        "\n",
        "# Enable bin packing (compaction) for a Delta Lake table\n",
        "book_table = DeltaTable.forPath(spark, \"/path-to-delta-table\")\n",
        "book_table.optimize().executeCompaction()\n"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "scala"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
