{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Transformations Using Spark in Synapse Analytics\n",
        "\n",
        "This notebook transforms trips data; converting it from CSV to Parquet format and splitting customer name into two separate fields.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "## Set variables\n",
        "\n",
        "import uuid\n",
        "\n",
        "# Variable for unique folder name\n",
        "target_folderName = \"tranformed\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Load source data\n",
        "\n",
        "Let's start by loading trips data into a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%%pyspark\n",
        "trips_df = spark.read.load('/trips/*.csv', format='csv', header=True, inferSchema=True\n",
        ")\n",
        "display(trips_df.limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Transform the data structure\n",
        "\n",
        "The source data includes a **CustomerName** field, that contains the customer's first and last name. Modify the dataframe to separate this field into separate **FirstName** and **LastName** fields."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import split, col\n",
        "\n",
        "# Create the new FirstName and LastName fields\n",
        "trips_df = trips_df.withColumn(\"FirstName\", split(col(\"customerName\"), \" \").getItem(0)).withColumn(\"LastName\", split(col(\"customerName\"), \" \").getItem(1))\n",
        "\n",
        "# Remove the CustomerName field\n",
        "trips_df = trips_df.drop(\"customerName\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract the new column **Year** from the **tripDate** field. Modify the dataframe to create the year field."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import substring\n",
        "\n",
        "trips_df = trips_df.withColumn('year', substring('tripDate', 7, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Save the transformed data\n",
        "\n",
        "Now save the transformed dataframe in Parquet format in a folder specified in a variable (Overwriting the data if it already exists)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "trips_df.write.mode(\"overwrite\").parquet('/%s' % target_folderName)\n",
        "print (\"Transformed data saved in %s!\" % target_folderName)"
      ]
    }
  ],
  "metadata": {
    "description": null,
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "save_output": true,
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
