{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f719de3a-1d57-4f33-a157-402c8b663948","showTitle":false,"title":""}},"source":["Use the following Azure Databricks storage setup block only if you are using Azure Databricks. You can refer to the instructions here to get started:\n","https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/azure-datalake-gen2-sp-access\n","\n","If you are using Synapse Spark and if your data is residing on the storage attached to the Synapse Spark workspace, you can skip the below storage setup section."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e7f5ea18-004c-4263-ad13-2fe218f26f10","showTitle":false,"title":""}},"outputs":[],"source":["%scala\n","val storageAccountName = \"<INSERT STORAGE ACCOUNT>\"\n","val fileSystemName = \"<INSERT CONTAINER NAME>\"\n","\n","val commonPath = \"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net\"\n","\n","# AAD Application Details\n","val appID = \"<INSERT APP ID>\"\n","val secret = \"<INSERT SECRET>\"\n","val tenantID = \"<INSERT TENANT ID>\"\n","\n","spark.conf.set(\"fs.azure.account.auth.type.\" + storageAccountName + \".dfs.core.windows.net\", \"OAuth\")\n","spark.conf.set(\"fs.azure.account.oauth.provider.type.\" + storageAccountName + \".dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.id.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + appID + \"\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.secret.\" + storageAccountName + \".dfs.core.windows.net\", \"\" + secret + \"\")\n","spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.\" + storageAccountName + \".dfs.core.windows.net\", \"https://login.microsoftonline.com/\" + tenantID + \"/oauth2/token\")\n","spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"true\")\n","dbutils.fs.ls(\"abfss://\" + fileSystemName  + \"@\" + storageAccountName + \".dfs.core.windows.net/\")\n","spark.conf.set(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e3c18e58-17b7-4e58-a9e7-08f48e0599c0","showTitle":false,"title":""}},"outputs":[],"source":["%scala\n","\n","// Let us see how to write to a JSON file\n","import org.apache.spark.sql.{DataFrame, Row, SaveMode}\n","import org.apache.spark.sql.types.{StringType, IntegerType, StructField, StructType}\n","\n","// Generate sample data\n","val driverDetails = Seq(\n","    Row(\"Alice\",\"\",\"Hood\",\"100\",\"New York\", \"Female\", 4100),\n","    Row(\"Bryan\",\"M\",\"Williams\",\"101\",\"New York\",\"Male\", 4000),\n","    Row(\"Catherine\",\"Goodwin\",\"\",\"102\",\"California\",\"Female\", 4300),\n","    Row(\"Daryl\",\"\",\"Jones\",\"103\",\"Florida\",\"Male\", 5500),\n","    Row(\"Jenny\",\"Anne\",\"Simons\",\"104\",\"Arizona\",\"Female\", 3400),\n","    Row(\"Daryl\",\"\",\"Jones\",\"103\",\"Florida\",\"Male\", 5500)\n","  )\n","\n","val driverSchema = new StructType().add(\"firstname\", StringType).add(\"middlename\", StringType).add(\"lastname\",StringType).add(\"id\",StringType).add(\"location\",StringType).add(\"gender\",StringType).add(\"salary\",IntegerType)\n","\n","// Create the Dataframe using the sample data\n","val df2 = spark.createDataFrame(spark.sparkContext.parallelize(driverDetails), driverSchema)\n","df2.printSchema()\n","df2.show(false)\n","\n","// Write to storage as JSON file\n","df2.write.mode(SaveMode.Overwrite).json(commonPath + \"/json/\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"98f9c0f1-7f41-43e6-9b5d-cc2c238f5668","showTitle":false,"title":""},"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["%scala\n","// Let us see how to read the JSON file back into a new Dataframe\n","\n","import org.apache.spark.sql.{DataFrame, Row, SaveMode}\n","import org.apache.spark.sql.types.{StringType, IntegerType, StructField, StructType}\n","\n","val driverSchema = new StructType().add(\"firstname\", StringType).add(\"middlename\", StringType).add(\"lastname\",StringType).add(\"id\",StringType).add(\"location\",StringType).add(\"gender\",StringType).add(\"salary\",IntegerType)\n","\n","val dfJSON = spark.read.schema(driverSchema).json(commonPath + \"/json/*.json\")\n","\n","// View the file\n","dfJSON.printSchema()\n","dfJSON.show(false)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"a6fb18ec-dcf0-4d2a-970c-c0b1df9537ff","showTitle":false,"title":""},"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[],"source":["%scala\n","import org.apache.spark.sql.{DataFrame, Row, SaveMode}\n","import org.apache.spark.sql.types.{StringType, IntegerType, StructField, StructType}\n","\n","// You can also infer the schema directly without specifying it for simple structures\n","\n","val dfJSON = spark.read.json(commonPath + \"/json/*.json\")\n","dfJSON.printSchema()\n","dfJSON.show(false)\n","\n","// Here is the deduced schema\n","val schema = dfJSON.schema\n","println(schema)"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":2},"notebookName":"ShreddingJSONUsingSpark-C8","notebookOrigID":188113580888548,"widgets":{}},"kernelspec":{"display_name":"python","name":"synapse_pyspark"},"language_info":{"name":"python"},"save_output":true,"synapse_widget":{"state":{},"version":"0.1"}},"nbformat":4,"nbformat_minor":0}
